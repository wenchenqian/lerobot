# æ•´ä½“æ¡†æ¶

![img_1.png](img_1.png)
æ•´ä½“æ¡†æ¶å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè®­ç»ƒå’Œæ¨ç†å„è‡ªæœ‰å…¶æ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶å¤¹ä¸‹æœ‰å¯¹åº”çš„è®­ç»ƒå’Œæ¨ç†apiï¼Œç„¶åcoreæ–‡ä»¶å¤¹å†…æœ‰è¿™æœ€æ ¸å¿ƒçš„ä¸€äº›ç»„ä»¶çš„å®ç°ï¼Œå¦‚optimizerã€modelsã€Transformerå’Œå„ç§å¹¶è¡Œå’Œåˆ†å¸ƒå¼é€šè®¯çš„åº“ã€‚

# å¹¶è¡Œåˆå§‹åŒ–

megatronçš„å¹¶è¡Œåˆå§‹åŒ–è°ƒç”¨çš„æ˜¯initialize_megatron()ï¼Œå†…éƒ¨ä¼šè°ƒç”¨initialize_model_parallel()ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå‡½æ•°æ˜¯å¦‚ä½•å¯¹DPã€PPã€TPã€CPã€EPè¿›è¡Œé€šè®¯åŸŸåˆå§‹åŒ–çš„ï¼š

```
""" å‡½æ•°çš„å‚æ•°å¤ªå¤šäº†ï¼Œè¿™é‡Œçœç•¥ """
def initialize_model_parallel(...) -> None:
    """ åˆå§‹åŒ–DPå¹¶è¡Œç»„ """
    ... # å…¨å±€å˜é‡è·å–
    for ranks in generator_wrapper('dp'):
        group = torch.distributed.new_group(ranks, timeout=timeout, pg_options=get_nccl_options('dp', nccl_comm_cfgs))
        group_gloo = torch.distributed.new_group(ranks, timeout=timeout, backend="gloo")
        if rank in ranks:
            """ 
            è¿™é‡Œçš„ifåˆ¤æ–­çš„é€»è¾‘æ˜¯ï¼šåªæœ‰æˆ‘è¿™ä¸ªè®­ç»ƒè¿›ç¨‹çš„rankåœ¨è¿™ä¸ªgroupé‡Œæˆ‘æ‰ä¼šæŠŠæˆ‘è¿™ä¸ªè®­ç»ƒè¿›ç¨‹çš„å¹¶è¡Œé€šä¿¡ç»„è®¾ç½®ä¸ºåˆšæ‰åˆ›å»ºå‡ºæ¥çš„groupã€‚
            è€Œè¿™ä¸ªgroupçš„ç±»å‹æ˜¯pytorchçš„ProcessGroupå¯¹è±¡ï¼Œå¯¹äºä¸‹é¢æ‰€æœ‰çš„å¹¶è¡Œç»„çš„åˆ›å»ºéƒ½æ˜¯èµ°çš„è¿™ä¸ªé€»è¾‘ã€‚
            """
            _DATA_PARALLEL_GROUP = group
            _DATA_PARALLEL_GROUP_GLOO = group_gloo
            _DATA_PARALLEL_GLOBAL_RANKS = ranks
  
    """ åˆå§‹åŒ–DP-CPå¹¶è¡Œç»„ï¼Œç”±äºCPçš„å¹¶è¡Œé€»è¾‘åŸå› ï¼ŒDPå’ŒCPçš„åˆ†ç»„å¯ä»¥å…±ç”¨ä¸€ä¸ªé€šä¿¡ç»„ """
    for ranks_with_cp in generator_wrapper('dp-cp'):
        group_with_cp = torch.distributed.new_group(ranks_with_cp, timeout=timeout, pg_options=get_nccl_options('dp_cp', nccl_comm_cfgs))
        group_with_cp_gloo = torch.distributed.new_group(ranks_with_cp, timeout=timeout, backend="gloo")
        if rank in ranks_with_cp:
             # do samething
   
    ...

    """ åˆå§‹åŒ–CPå¹¶è¡Œç»„ """
    ... # å…¨å±€å˜é‡è·å–
    for ranks in generator_wrapper('cp'):
        group = torch.distributed.new_group(ranks, timeout=timeout, pg_options=get_nccl_options('cp', nccl_comm_cfgs))
        if rank in ranks:
            # do samething

    """ åˆå§‹åŒ–DPåŸŸå¹¶è¡Œç»„ï¼Œè¿™ä¸ªgroupå†…åŒ…å«äº†ä¸€ä¸ªå®Œæ•´æ¨¡å‹å‚æ•°æ‰€éœ€çš„rankå· """
    ... # å…¨å±€å˜é‡è·å–
    for ranks in generator_wrapper('tp-pp'):
        group = torch.distributed.new_group(ranks, timeout=timeout, pg_options=get_nccl_options('mp', nccl_comm_cfgs))
        if rank in ranks:
            # do samething
    ...
   
    """ åˆå§‹åŒ–TPå¹¶è¡Œç»„ """
    ... # å…¨å±€å˜é‡è·å–
    for ranks in generator_wrapper('tp'):
        group = torch.distributed.new_group(ranks, timeout=timeout, pg_options=get_nccl_options('tp', nccl_comm_cfgs))
        if rank in ranks:
            # do samething

    """ åˆå§‹åŒ–PPå¹¶è¡Œç»„ """
    ... # å…¨å±€å˜é‡è·å–
    for ranks in generator_wrapper('pp'):
        group = torch.distributed.new_group(ranks, timeout=timeout, pg_options=get_nccl_options('pp', nccl_comm_cfgs))
        if rank in ranks:
            if _PIPELINE_MODEL_PARALLEL_GROUP is None:
                _PIPELINE_MODEL_PARALLEL_GROUP = group
                _PIPELINE_GLOBAL_RANKS = ranks
            elif isinstance(_PIPELINE_GLOBAL_RANKS[0], list):
                _PIPELINE_MODEL_PARALLEL_GROUP.append(group)
                _PIPELINE_GLOBAL_RANKS.append(ranks)
            else:
                _PIPELINE_MODEL_PARALLEL_GROUP = [_PIPELINE_MODEL_PARALLEL_GROUP, group]
                _PIPELINE_GLOBAL_RANKS = [_PIPELINE_GLOBAL_RANKS, ranks]

        embedding_ranks = get_embedding_ranks(ranks)
        group = torch.distributed.new_group(embedding_ranks, timeout=timeout, pg_options=get_nccl_options('embd', nccl_comm_cfgs))
        if rank in embedding_ranks:
            # do samething

        position_embedding_ranks = get_position_embedding_ranks(ranks)
        group = torch.distributed.new_group(...)
        if rank in position_embedding_ranks:
            # do samething
   
    """ åˆå§‹åŒ–TP+CPå¹¶è¡Œç»„ï¼Œè¿™ä¸ªå¹¶è¡Œç»„ç”¨äºæå‡æ•ˆç‡ï¼Œå› ä¸ºAttentionå±‚å‰TPå’ŒCPå¹¶è¡Œç»„éƒ½ä¼šè°ƒç”¨all-gatherï¼Œæ‰€ä»¥å¯ä»¥åˆå¹¶ """
    ... # å…¨å±€å˜é‡è·å–
    for ranks in generator_wrapper('tp-cp'):
        group = torch.distributed.new_group(ranks, timeout=timeout, pg_options=get_nccl_options('tp_cp', nccl_comm_cfgs))
        if rank in ranks:
           # do samething
  
    ... """ EPï¼ŒTP+EPç­‰ç­‰ """ 

    _set_global_memory_buffer()
```

### Megatron å¦‚ä½•åˆ†ç»„ï¼Ÿâ€”â€” ä»¥ TP=2, PP=2, DP=2 ä¸ºä¾‹ï¼ˆå…± 8 GPUï¼‰

#### ğŸ“Œ æ­¥éª¤ 1ï¼šæŒ‰ PP åˆ’åˆ† â€”â€” 2 ä¸ªæµæ°´ stage

* Stage 0: GPU [0, 1, 2, 3]
* Stage 1: GPU [4, 5, 6, 7]

#### ğŸ“Œ æ­¥éª¤ 2ï¼šåœ¨æ¯ä¸ª stage å†…ï¼ŒæŒ‰ DP åˆ’åˆ† â€”â€” 2 ä¸ªæ•°æ®å¹¶è¡Œç»„

* Stage 0:
  * DP Group 0: [0, 2]
  * DP Group 1: [1, 3]
* Stage 1:
  * DP Group 0: [4, 6]
  * DP Group 1: [5, 7]

#### ğŸ“Œ æ­¥éª¤ 3ï¼šåœ¨æ¯ä¸ª DP ç»„å†…ï¼ŒæŒ‰ TP åˆ’åˆ† â€”â€” 2 ä¸ªå¼ é‡å¹¶è¡Œç»„

* Stage 0, DP Group 0: [0, 2] â†’ TP ç»„å°±æ˜¯ [0, 2]ï¼ˆTP=2ï¼Œåˆšå¥½2ä¸ªï¼‰
* Stage 0, DP Group 1: [1, 3] â†’ TP ç»„æ˜¯ [1, 3]
* Stage 1, DP Group 0: [4, 6] â†’ TP ç»„ [4, 6]
* Stage 1, DP Group 1: [5, 7] â†’ TP ç»„ [5, 7]

## data\_iteratoråˆ›å»º

åœ¨å½“å‰ç»å¤§å¤šæ•°åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬éƒ½æ˜¯ç”¨çš„gptæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¯¹åº”çš„megatronä¹Ÿæœ‰`GPTDataset`ï¼Œæˆ‘ä»¬åœ¨å¯åŠ¨è®­ç»ƒå‰å°±ä¼šåˆ›å»ºå‡º`data_iterator`å’Œ`Dataloader`ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹megatronæ˜¯å¦‚ä½•åˆ›å»ºè¿™äº›å…³é”®ç»„ä»¶çš„

```python
def train_valid_test_datasets_provider(train_val_test_num_samples):
    args = get_args()

    config = core_gpt_dataset_config_from_args(args)

    if config.mock:
        dataset_type = MockGPTDataset
    else:
        dataset_type = GPTDataset   """ ä½¿ç”¨çš„æ•°æ®é›†ç±»å‹ """
    print_rank_0("> building train, validation, and test datasets for GPT ...")

    if args.is_instruction_dataset:
        ...  """ å¾ˆå°‘ç”¨ """
    else:
        """ å…³é”®å‡½æ•° """
        train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(dataset_type, train_val_test_num_samples, is_dataset_built_on_rank, config).build()

    print_rank_0("> finished creating GPT datasets ...")

    return train_ds, valid_ds, test_ds
```

ä¸Šé¢çš„ä»£ç è¿›å…¥åˆ°äº†BlendedMegatronDatasetBuilderå†…è¿›è¡Œæ•°æ®é›†åˆ›å»ºï¼Œè¿™å†…éƒ¨çš„ä»£ç ååˆ†å†—é•¿ï¼Œé€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œæ•…æ”¾åˆ°è¿›é˜¶å†…å®¹çš„7.1èŠ‚è¿›è¡Œè¯¦ç»†è®²è¿°ã€‚æ¥ä¸‹æ¥ç²—ç•¥è®²è¿°ä¸€ä¸‹data-iteratorçš„åˆ›å»º

```python
def pretrain(...):
    # Initalize and get arguments, timers, and Tensorboard writer.
    initialize_megatron(...) 
    ...
    # Model, optimizer, and learning rate.
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(model_provider, model_type)
    ...
    # Data stuff.
    if args.virtual_pipeline_model_parallel_size is not None:
        train_data_iterator = []
        valid_data_iterator = []
        test_data_iterator = []
        for i in range(len(model)):
            mpu.set_virtual_pipeline_model_parallel_rank(i)
            iterators = build_train_valid_test_data_iterators(
                train_valid_test_dataset_provider)
            train_data_iterator.append(iterators[0])
            valid_data_iterator.append(iterators[1])
            test_data_iterator.append(iterators[2])
    else:
        """ VPPæœªå¯ç”¨æ—¶çš„é€»è¾‘ï¼Œæˆ‘ä»¬å¤§å¤šæ•°æ—¶å€™éƒ½èµ°è¿™é‡Œ """
        train_data_iterator, valid_data_iterator, test_data_iterator = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)
    print_datetime('after dataloaders are built')


def build_train_valid_test_data_iterators(build_train_valid_test_datasets_provider):
    args = get_args()

    """ ä½¿ç”¨æˆ‘ä»¬è¿™ä¸€ç« èŠ‚æœ€å¼€å§‹ç»™å‡ºçš„é‚£ä¸ªdataset providerå‡½æ•°åˆ›å»ºæ•°æ®é›†å’ŒDataloader """
    train_dataloader, valid_dataloader, test_dataloader = build_train_valid_test_data_loaders(build_train_valid_test_datasets_provider)

    # Build iterators.
    dl_type = args.dataloader_type
    assert dl_type in ['single', 'cyclic', 'external']

    def _get_iterator(dataloader_type, dataloader):
        """Return dataset iterator."""
        if dataloader_type == "single":
            return iter(dataloader)
        elif dataloader_type == "cyclic":
            return iter(cyclic_iter(dataloader))
        elif dataloader_type == "external":
            # External dataloader is passed through. User is expected to define how to iterate.
            return dataloader
        else:
            raise RuntimeError("unexpected dataloader type")

    if train_dataloader is not None:
        train_data_iterator = _get_iterator(dl_type, train_dataloader)
    else:
        train_data_iterator = None
    if valid_dataloader is not None:
        valid_data_iterator = _get_iterator(dl_type, valid_dataloader)
    else:
        valid_data_iterator = None
    if test_dataloader is not None:
        test_data_iterator = _get_iterator(dl_type, test_dataloader)
    else:
        test_data_iterator = None
    """ è¿™ä¸ªiteratorå°±æ˜¯å°†DataloaderåŒ…è£…ä¸ºå¯è¿­ä»£çš„å¯¹è±¡ï¼Œä»¥æ–¹ä¾¿åœ¨gpt_forward_stepä¸­ä½¿ç”¨next(data_iterator)è·å¾—æ•°æ® """ 
    return train_data_iterator, valid_data_iterator, test_data_iterator
```

ä»ä¸Šé¢å¯ä»¥çœ‹åˆ°åˆ›å»ºçš„è¿‡ç¨‹ï¼Œå…¶ä¸­Dataloaderçš„åˆ›å»ºæˆ‘è¿™é‡Œçœç•¥ï¼Œå› ä¸ºè¿™é‡Œè¯»è€…ä»¬è‡ªå·±å¯ä»¥å»é˜…è¯»ä¸€ä¸‹ä»£ç ï¼Œå…¶å®å·²ç»å¾ˆå®¹æ˜“å°±èƒ½å¤Ÿä¸²èµ·å¼€å§‹çš„datasetåˆ›å»ºå’Œä¸Šé¢ç»™å‡ºçš„iteratoråˆ›å»ºçš„è¿‡ç¨‹ã€‚

åœ¨Dataloaderåˆ›å»ºçš„å‡½æ•°ä¸­ä¼šå…ˆåˆ›å»ºå‡ºä¸‰ä¸ªdatasetã€trainï¼Œvalidï¼Œtestã€‘ï¼Œç„¶åæ ¹æ®ä¸‰ä¸ªdatasetå„è‡ªåˆ›å»ºå‡ºå¯¹åº”çš„Dataloaderã€è¿™é‡Œçš„Dataloaderå°±æ˜¯pytorchçš„Dataloaderã€‘ã€‚

æœ‰äº†dataloaderä¹‹åæˆ‘ä»¬å°±å¯ä»¥åœ¨è®­ç»ƒä¸­è¿›è¡Œbatchæ•°æ®è¯»å–äº†ï¼Œè¿™é‡Œçš„é€»è¾‘ç¬”è€…æ”¾åœ¨ä¸‹ä¸€ç« çš„é¢„è®­ç»ƒä¸­è®²è¿°ã€‚

# é¢„è®­ç»ƒï¼š

## åŸºäºiterationçš„è®­ç»ƒï¼š

ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åŸºäºepochæ¥è®­ç»ƒçš„æ¨¡å¼ä¸åŒï¼ŒLLMå†…æ²¡æœ‰äº†epochçš„æ¦‚å¿µï¼Œåªæœ‰`iteration`çš„æ¦‚å¿µâ€”â€”å³è¿™æ¬¡è®­ç»ƒä¼šè¿›è¡ŒN\*Mä¸ª`forward-backward`æ“ä½œï¼ŒNä¸ª`step`æ“ä½œã€‚ï¼ˆ`N = iteration`ï¼Œ`M = global_batch_size / micro_batch_size`ï¼‰

ä¸‹é¢æˆ‘ä»¬ä»ä»£ç ä¸Šçœ‹çœ‹åŸºäºiterationçš„è®­ç»ƒä»£ç ï¼š

```python
def train(forward_step_func, model, optimizer, opt_param_scheduler, train_data_iterator, valid_data_iterator, process_non_loss_data_func, config):
    args = get_args()
    timers = get_timers()

    # Turn on training mode which enables dropout.
    for model_module in model:
        model_module.train()

    # Tracking loss.
    total_loss_dict = {}

    # Iterations.
    iteration = args.iteration

    ... # Setup some training config params
  
    """ æœ€é‡è¦çš„whileè®­ç»ƒï¼Œé‡Œé¢æ‰§è¡Œiterationæ¬¡train_stepå‡½æ•° """
    num_microbatches = get_num_microbatches()
    while iteration < args.train_iters:
        ... """ æ›´æ–°num microbatchå¤§å° """

        args.curr_iteration = iteration
        """ è°ƒç”¨train_stepè®­ç»ƒä¸€ä¸ªstepã€fwdã€bwdã€optimizer.stepã€‘ """
        loss_dict, skipped_iter, grad_norm, num_zeros_in_grad = \
            train_step(forward_step_func,
                       train_data_iterator,
                       model,
                       optimizer,
                       opt_param_scheduler,
                       config)
        iteration += 1
        batch_size = mpu.get_data_parallel_world_size() * \
                     args.micro_batch_size * \
                     get_num_microbatches()
        args.consumed_train_samples += batch_size
  
        ... """ loggingç›¸å…³ä»£ç  """

        ... """ Evaluation """
  
        ... """ Checkpointing ä¿å­˜æƒé‡æ–‡ä»¶ """ 
  
    ... """ æ‰€æœ‰iterationå®Œæˆä¹‹åçš„é€€å‡ºé€»è¾‘ """

    return iteration, num_floating_point_operations_so_far
```

## train\_stepå‡½æ•°ï¼š

åœ¨megatronä¸­ï¼Œ`forward`å’Œ`backward`å› ä¸ºæµæ°´çº¿å¹¶è¡Œè€Œéœ€è¦å®šåˆ¶ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸èƒ½åƒåŸå§‹çš„æœºå™¨å­¦ä¹ ä¸€æ ·ç›´æ¥æŠŠinputæ”¾å…¥modelï¼Œç„¶åè°ƒç”¨`loss.backward()`è®¡ç®—æ¢¯åº¦ã€‚åŸå› å°±æ˜¯æ¯å¼ å¡é‡Œçš„modelå…¶å®éƒ½ä¸ä¸€æ ·ï¼Œå¯¹äºè¾“å…¥çš„shapeçš„è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œ

æ‰€ä»¥æˆ‘ä»¬éœ€è¦åŒ…è£…`forward`çš„inputï¼Œæœ‰å¯èƒ½è¿™ä¸ªinputæ˜¯æœ€å¼€å§‹çš„è¾“å…¥ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ä¸Šä¸€ä¸ªpipeline stageçš„outputã€‚æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹å¯¹äºæµæ°´çº¿å¹¶è¡Œæ¥è¯´`forward`å’Œ`backward`çš„æµç¨‹ï¼š

```python
def train_step(...):
    ...
    """ æ¸…é™¤grad """
    if args.DDP_impl == 'local' and args.use_contiguous_buffers_in_local_ddp:
        for partition in model:
            partition.zero_grad_buffer()
    optimizer.zero_grad()
    ...
  
    """ æ‰§è¡Œå‰å‘ã€åå‘è®¡ç®— """
    forward_backward_func = get_forward_backward_func()  """ è·å¾—forwardå’Œbackwardå‡½æ•°ï¼Œé‡Œé¢ä¼šæ ¹æ®PPçš„è®¾ç½®è¿”å›ä¸åŒçš„å‡½æ•° """
    losses_reduced = forward_backward_func(...) 
    ...
  
    """ å¯¹æ¢¯åº¦æ‰§è¡ŒReduce-Scatteræ“ä½œ """
    optimizer.reduce_model_grads(args, timers)
    ...
  
    """ æ›´æ–°è¯¥DPåŸŸæŒæœ‰çš„æ¢¯åº¦å¯¹åº”çš„éƒ¨åˆ†å‚æ•° """
    timers('optimizer', log_level=1).start(barrier=args.barrier_with_L1_time)
    update_successful, grad_norm, num_zeros_in_grad = optimizer.step(args, timers)
    timers('optimizer').stop()
    ...
  
    """ å¯¹æ›´æ–°åçš„paramæ‰§è¡Œgatheræ“ä½œ """
    if update_successful:
        optimizer.gather_model_params(args, timers)
    ...
  
    """ é€šè¿‡scheduleræ›´æ–°å­¦ä¹ ç‡ """
    if update_successful:
        increment = get_num_microbatches() * args.micro_batch_size * args.data_parallel_size
        opt_param_scheduler.step(increment=increment)
        skipped_iter = 0
    else:
        skipped_iter = 1
    ...
```

åœ¨å°†fwdå’Œbwdä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰å¿…è¦äº†è§£ä¸€ä¸‹global_batch_size, micro_batch_size, DP_sizeå’Œnum_micro_batchesçš„å…³ç³»ï¼š

```python
 self.num_micro_batches = (running_global_batch_size // micro_batch_times_data_parallel_size)  # 16 // (2 * 4) = 2
```

æˆ‘ä»¬åœ¨è®­ç»ƒçš„è„šæœ¬é‡Œä¼šæŒ‡å®šè¿™ä¸¤ä¸ªsizeå‚æ•°ï¼Œç„¶åmegatronä¼šæ ¹æ®DPçš„æ•°é‡è®¡ç®—å‡ºæ¥è¿™ä¸ªnum_micro_batchesçš„å¤§å°ï¼Œè¿™ä¸ªæ•°çš„ç”¨å¤„å¾ˆå¤§ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†fwd-bwdçš„æ—¶å€™å°±ä¼šè®²åˆ°ã€‚

```python
def get_forward_backward_func():
    pipeline_model_parallel_size = parallel_state.get_pipeline_model_parallel_world_size()
    if pipeline_model_parallel_size > 1:
        if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
            forward_backward_func = forward_backward_pipelining_with_interleaving
        else:
            forward_backward_func = forward_backward_pipelining_without_interleaving   """ ç€é‡çœ‹VPPä¸è®¾ç½®çš„æƒ…å†µä¸‹çš„fwd-bwdå‡½æ•° """
    else:
        forward_backward_func = forward_backward_no_pipelining
    return forward_backward_func


def forward_backward_pipelining_without_interleaving(......):
    ... """ çœç•¥çš„æ˜¯ä¸€äº›æ¢¯åº¦åŒæ­¥ç›¸å…³çš„è®¾ç½®ä»¥åŠassert """

    """ microbatchesè®¡ç®— """
    num_warmup_microbatches = (parallel_state.get_pipeline_model_parallel_world_size() - parallel_state.get_pipeline_model_parallel_rank() - 1)
    num_warmup_microbatches = min(num_warmup_microbatches, num_microbatches)
    num_microbatches_remaining = num_microbatches - num_warmup_microbatches

    if config.num_microbatches_with_partial_activation_checkpoints is not None:
        max_outstanding_backprops = num_warmup_microbatches + 1

    model_type = get_model_type(model)

    rank = parallel_state.get_pipeline_model_parallel_rank()
    ...

    # Input, output tensors only need to be saved when doing backward passes
    input_tensors = None
    output_tensors = None
    if not forward_only:
        input_tensors = []
        output_tensors = []
    forward_data_store = []

    """ æ³¨æ„ï¼ï¼Œè¿™é‡Œéƒ½æ˜¯å¾ªç¯ï¼Œæ¯æ¬¡å¾ªç¯éƒ½ä¼šè°ƒç”¨ä¸€æ¬¡forwardï¼Œæ‰€ä»¥ä¸€ä¸ªiterationä¸åªä¼šè°ƒç”¨ä¸€æ¬¡fwd-bwdï¼Œè€Œæ˜¯ä¼šå¾ªç¯è°ƒç”¨num_micro_batchesæ¬¡ã€‚""" 

    """ Run warmup forward passes. """
    for i in range(num_warmup_microbatches):
        ...
        """ ä»ä¸Šä¸€ä¸ªstageè·å¾—input tensor ã€å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªstageåˆ™ç›´æ¥è·å¾—è¾“å…¥ã€‘"""
        input_tensor = recv_forward(recv_tensor_shapes, config)
        """ æ‰§è¡Œforwardå‡½æ•°ï¼Œå¹¶å°†outputå‘é€ç»™ä¸‹ä¸€ä¸ªstage """
        output_tensor = forward_step()
        send_forward(output_tensor, send_tensor_shapes, config)

        if not forward_only:
            input_tensors.append(input_tensor)
            output_tensors.append(output_tensor)
            deallocate_output_tensor(output_tensor[0], config.deallocate_pipeline_outputs)

    """ 
    å®Œæˆçƒ­èº«forwardä¹‹åå¼€å§‹fwd-bwdï¼Œä¸Šé¢çš„çƒ­èº«å…¶å®å°±æ˜¯å¡«å……æµæ°´çº¿çš„æ„æ€ï¼š
    æˆ‘ä»¬æƒ³è¦æ‰§è¡Œbwdï¼Œé‚£ä¹ˆé¦–å…ˆå¾—ç¬¬ä¸€ä¸ªbatchçš„æ•°æ®æ‰§è¡Œå®Œæ•´ä¸ªfwdæµç¨‹ï¼Œå¡«å……å®Œä¹‹åæ¯ä¸ªstageå°±è¿›å…¥äº†ä¸€ä¸ªfwdä¸€ä¸ªbwdçš„ç¨³å®šé˜¶æ®µ.
    """
    # Before running 1F1B, need to receive first forward tensor. If all microbatches are run in warmup / cooldown phase, then no need to receive this tensor here.
    if num_microbatches_remaining > 0:
        input_tensor = recv_forward(recv_tensor_shapes, config)

    """ Run 1F1B in steady state.  """
    for i in range(num_microbatches_remaining):
        last_iteration = i == (num_microbatches_remaining - 1)
        ...
        """ æ‰§è¡Œforwardå‡½æ•°ï¼Œå¹¶å°†outputå‘é€ç»™ä¸‹ä¸€ä¸ªstage """
        output_tensor = forward_step(...)

        """ forward_onlyè¡¨ç¤ºæ¨ç†åœºæ™¯ """
        if forward_only:
            send_forward(output_tensor, send_tensor_shapes, config)

            if not last_iteration:
                input_tensor = recv_forward(recv_tensor_shapes, config)
        else:
            """ 
            è®­ç»ƒåœºæ™¯ä¸‹å°†å‘é€forwardçš„outputå’Œæ¥æ”¶backwardçš„gradåˆå¹¶ã€‚
            å› ä¸ºè¿™é‡Œæ˜¯çƒ­èº«ä¹‹åçš„åœºæ™¯ï¼Œæµæ°´çº¿å·²ç»è¢«å¡«å……ï¼Œæ¯ä¸ªforward stageå®Œæˆå°±å¯¹åº”äº†ä¸€ä¸ªbackward stageçš„å®Œæˆ 
            """
            output_tensor_grad = send_forward_recv_backward(output_tensor, send_tensor_shapes, config)

            # Add input_tensor and output_tensor to end of list.
            input_tensors.append(input_tensor)
            output_tensors.append(output_tensor)
            deallocate_output_tensor(output_tensor[0], config.deallocate_pipeline_outputs)

            # Pop input_tensor and output_tensor from the start of the list for the backward pass.
            input_tensor = input_tensors.pop(0)
            output_tensor = output_tensors.pop(0)

            # Enable grad sync for the last microbatch in the batch if the full backward pass completes in the 1F1B stage.
            if num_warmup_microbatches == 0 and last_iteration:
                if config.grad_sync_func is None or rank == 0:
                    enable_grad_sync()
  
            """
            æ‰§è¡Œbackwardï¼Œå¯¹åº”çš„æ¢¯åº¦è®¡ç®—éœ€è¦ç”¨åˆ°å‰é¢è·å–åˆ°çš„outputçš„æ¢¯åº¦ã€ä¹Ÿå³ä¸‹ä¸€ä¸ªstageè®¡ç®—backwardçš„æ¢¯åº¦ä»¥åŠæœ¬stageä¸­forwardçš„output_tensorã€‘
            æœ‰äº†è¿™ä¸‰ä¸ªæ•°æ®åä¾¿å¯ä»¥è®¡ç®—å‡ºæœ¬stageçš„input_tensorçš„æ¢¯åº¦ï¼Œç„¶åæŠŠè¿™ä¸ªæ¢¯åº¦åˆå‘é€ç»™ä¸Šä¸€ä¸ªstageã€å¦‚æœæœ‰çš„è¯ã€‘ã€‚
            """
            input_tensor_grad = backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)

            if last_iteration:
                input_tensor = None
                send_backward(input_tensor_grad, recv_tensor_shapes, config)
            else:
                input_tensor = send_backward_recv_forward(input_tensor_grad, recv_tensor_shapes, config)

    """ Run cooldown backward passes. æŠŠæµæ°´çº¿ä¸­å‰©ä¸‹çš„backward stageå®Œæˆ """
    if not forward_only:
        for i in range(num_warmup_microbatches):
            if i == num_warmup_microbatches - 1:
                if config.grad_sync_func is None or rank == 0:
                    enable_grad_sync()

            input_tensor = input_tensors.pop(0)
            output_tensor = output_tensors.pop(0)

            output_tensor_grad = recv_backward(send_tensor_shapes, config)

            input_tensor_grad = backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)

            send_backward(input_tensor_grad, recv_tensor_shapes, config)

    ...  """ æ¢¯åº¦åŒæ­¥ã€è®¡æ—¶å’Œæ¸…ç† """

    return forward_data_store
```

> `forward_step()` ç”¨çš„ `input_tensor` è¢« `append` åˆ° `input_tensors` åˆ—è¡¨æœ«å°¾
> `backward_step()` ç”¨çš„ `input_tensor` æ˜¯ä» `input_tensors` **å‰ç«¯ `pop`** å‡ºæ¥çš„

> **å› ä¸ºåœ¨æµæ°´å¹¶è¡Œä¸­ï¼Œåå‘ä¼ æ’­çš„æ‰§è¡Œé¡ºåºä¸å‰å‘ä¼ æ’­æ˜¯â€œé•œåƒç›¸åâ€çš„ â€”â€” æœ€åä¸€ä¸ª stage æœ€å…ˆå¼€å§‹åå‘ï¼Œç¬¬ä¸€ä¸ª stage æœ€ååå‘ã€‚å› æ­¤ï¼Œæ¿€æ´»å€¼ï¼ˆinput_tensorï¼‰å¿…é¡»æŒ‰â€œå…ˆè¿›å…ˆå‡ºâ€ï¼ˆFIFOï¼‰é¡ºåºç®¡ç†ï¼Œæ‰èƒ½ä¿è¯åå‘æ—¶æ‹¿åˆ°æ­£ç¡®çš„ã€å¯¹åº” microbatch çš„è¾“å…¥ã€‚**

##### æµæ°´å¹¶è¡Œä¸­çš„ Microbatch æ‰§è¡Œé¡ºåº

å‡è®¾ï¼š

- æµæ°´çº¿æœ‰ 4 ä¸ª stageï¼ˆS0, S1, S2, S3ï¼‰
- æœ‰ 4 ä¸ª microbatchï¼ˆMB0, MB1, MB2, MB3ï¼‰

å‰å‘ä¼ æ’­ï¼ˆForwardï¼‰æ‰§è¡Œé¡ºåºï¼ˆæ—¶é—´æ­¥ï¼‰ï¼š

â†’ **å‰å‘é¡ºåºï¼šæ¯ä¸ª stage æŒ‰ MB0 â†’ MB1 â†’ MB2 â†’ MB3 é¡ºåºå¤„ç†**

---

#### åå‘ä¼ æ’­ï¼ˆBackwardï¼‰æ‰§è¡Œé¡ºåº

åå‘ä¼ æ’­å¿…é¡»ä»**æœ€åä¸€ä¸ª stageï¼ˆS3ï¼‰å¼€å§‹**ï¼Œå› ä¸ºï¼š

- æŸå¤±å‡½æ•°åœ¨ S3 è¾“å‡ºåè®¡ç®—
- æ¢¯åº¦ä» S3 å¼€å§‹åå‘ä¼ æ’­ â†’ S2 â†’ S1 â†’ S0

è€Œä¸”ï¼Œä¸ºäº†åŒ¹é…å‰å‘çš„ microbatchï¼Œåå‘ä¹Ÿå¿…é¡»æŒ‰ microbatch é¡ºåºè¿›è¡Œã€‚

ä½†æ³¨æ„ï¼š**S3 æœ€å…ˆå®Œæˆ MB0 çš„å‰å‘ â†’ æ‰€ä»¥å®ƒæœ€å…ˆå¼€å§‹ MB0 çš„åå‘ï¼**

### ğŸ”„ åå‘ä¼ æ’­æ‰§è¡Œé¡ºåºï¼ˆç†æƒ³åŒæ­¥æƒ…å†µï¼‰ï¼š

â†’ **åå‘é¡ºåºï¼šæ¯ä¸ª stage æŒ‰ MB0 â†’ MB1 â†’ MB2 â†’ MB3 é¡ºåºå¤„ç†ï¼ˆå’Œå‰å‘ä¸€è‡´ï¼‰**

BUT â€”â€” æ³¨æ„ï¼š**S3 å…ˆåå‘ MB0ï¼ŒS2 åœ¨ç¨ååå‘ MB0ï¼ŒS1 æ›´æ™šï¼ŒS0 æœ€æ™šã€‚**

---

#### æ¯ä¸ª stage å¦‚ä½•ä¿å­˜å’Œå–å‡º input_tensorï¼Ÿ

åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œæ¯ä¸ª stage å¯¹æ¯ä¸ª microbatch éƒ½ä¼šï¼š

1. æ¥æ”¶ `input_tensor`ï¼ˆæ¥è‡ªä¸Šä¸€ stage æˆ–æ•°æ®åŠ è½½å™¨ï¼‰
2. è®¡ç®— `output_tensor`
3. **æŠŠ `input_tensor` ä¿å­˜èµ·æ¥** â†’ å› ä¸ºåå‘ä¼ æ’­æ—¶éœ€è¦å®ƒæ¥è®¡ç®—æ¢¯åº¦ï¼ˆå°¤å…¶æ˜¯ç”¨äº†æ¿€æ´»é‡è®¡ç®—æ—¶ï¼‰
4. æŠŠ `output_tensor` å‘é€ç»™ä¸‹ä¸€ stage

â†’ æ‰€ä»¥æ¯ä¸ª stage éƒ½éœ€è¦ä¸€ä¸ª **é˜Ÿåˆ—ï¼ˆlistï¼‰** æ¥å­˜å‚¨å®ƒå¤„ç†è¿‡çš„æ‰€æœ‰ microbatch çš„ `input_tensor`

---

#### ä¸ºä»€ä¹ˆæ˜¯ append + pop(0)ï¼Ÿâ€”â€” FIFOï¼

ç°åœ¨æˆ‘ä»¬èšç„¦åœ¨ä¸€ä¸ª stage ä¸Šï¼Œæ¯”å¦‚ **S1**ã€‚

### å‰å‘æ—¶ï¼ˆæŒ‰é¡ºåºå¤„ç† MB0, MB1, MB2, MB3ï¼‰ï¼š

```python
input_tensors = []

# å¤„ç† MB0
input_tensor_0 = receive_from_S0()
output_tensor_0 = forward_step(input_tensor_0)
input_tensors.append(input_tensor_0)   # â†’ [inp0]

# å¤„ç† MB1
input_tensor_1 = receive_from_S0()
output_tensor_1 = forward_step(input_tensor_1)
input_tensors.append(input_tensor_1)   # â†’ [inp0, inp1]

# å¤„ç† MB2
input_tensors.append(input_tensor_2)   # â†’ [inp0, inp1, inp2]

# å¤„ç† MB3
input_tensors.append(input_tensor_3)   # â†’ [inp0, inp1, inp2, inp3]
```

### åå‘æ—¶ï¼ˆS1 å¿…é¡»åœ¨ S2 å¼€å§‹åå‘ MB0 ä¹‹åï¼Œæ‰èƒ½å¼€å§‹åå‘ MB0ï¼‰ï¼š

```python
# å½“ S2 å¼€å§‹åå‘ MB0 æ—¶ï¼Œå®ƒä¼šæŠŠæ¢¯åº¦ä¼ ç»™ S1
# S1 éœ€è¦ MB0 çš„ input_tensor æ¥è®¡ç®—è‡ªå·±çš„æ¢¯åº¦

input_tensor_for_bw = input_tensors.pop(0)  # â† å–å‡º inp0
grad_output = receive_grad_from_S2()        # MB0 çš„æ¢¯åº¦
grad_input = backward_step(input_tensor_for_bw, grad_output)
send_grad_to_S0(grad_input)
```

â†’ ç„¶åæ˜¯ MB1ï¼š

```python
input_tensor_for_bw = input_tensors.pop(0)  # â† å–å‡º inp1ï¼ˆç°åœ¨åˆ—è¡¨æ˜¯ [inp2, inp3]ï¼‰
...
```

ğŸ“Œ **å¿…é¡»æŒ‰ MB0, MB1, MB2, MB3 çš„é¡ºåºåå‘ â†’ æ‰€ä»¥å¿…é¡»æŒ‰â€œå…ˆè¿›å…ˆå‡ºâ€é¡ºåºå–å‡º input_tensor**

---

#### input_tensors é˜Ÿåˆ—å˜åŒ–

```
å‰å‘ï¼š
  MB0 â†’ append â†’ [inp0]
  MB1 â†’ append â†’ [inp0, inp1]
  MB2 â†’ append â†’ [inp0, inp1, inp2]
  MB3 â†’ append â†’ [inp0, inp1, inp2, inp3]

åå‘ï¼š
  BW MB0 â†’ pop(0) â†’ inp0, å‰©ä¸‹ [inp1, inp2, inp3]
  BW MB1 â†’ pop(0) â†’ inp1, å‰©ä¸‹ [inp2, inp3]
  BW MB2 â†’ pop(0) â†’ inp2, å‰©ä¸‹ [inp3]
  BW MB3 â†’ pop(0) â†’ inp3, å‰©ä¸‹ []
```

#### ä¸ºä»€ä¹ˆ output_tensors ä¹Ÿæ˜¯ FIFOï¼Ÿ

ä½ æåˆ°è¿˜æœ‰ `output_tensors` â€”â€” å®ƒé€šå¸¸ç”¨äºï¼š

- åœ¨åå‘ä¼ æ’­æ—¶ï¼ŒæŠŠå‰å‘çš„ `output_tensor` ä¼ ç»™ `torch.autograd.backward()`ï¼ˆå¦‚æœæ²¡ç”¨æ¿€æ´»é‡è®¡ç®—ï¼‰

å®ƒçš„ä½¿ç”¨é¡ºåºå’Œ `input_tensors` ä¸€è‡´ â€”â€” **åå‘æ—¶ä¹Ÿéœ€è¦æŒ‰ MB0, MB1... é¡ºåºå–å‡ºå¯¹åº”çš„ output_tensor**

```python
output_tensors.append(output_tensor)   # forward æ—¶
...
output_tensor_for_bw = output_tensors.pop(0)  # backward æ—¶
```

## forward\_stepï¼š

```python
""" ä½äºPIPELINEå†…çš„forward_stepï¼Œä¸‹é¢é‚£ä¸ªçŸ­çš„forward_stepæ‰æ˜¯æœ€ç»ˆçš„ """
def forward_step(......):
    ... """ timers and set first microbatch """ 

    unwrap_output_tensor = False
    if not isinstance(input_tensor, list):
        input_tensor = [input_tensor]
        unwrap_output_tensor = True

    set_input_tensor = get_attr_wrapped_model(model, "set_input_tensor")
    set_input_tensor(input_tensor)

    if config.enable_autocast:
        context_manager = torch.autocast("cuda", dtype=config.autocast_dtype)
    else:
        context_manager = contextlib.nullcontext()
    with context_manager:
        """ æ‰§è¡Œå‰å‘ä¼ æ’­è®¡ç®— """
        if checkpoint_activations_microbatch is None:
            output_tensor, loss_func = forward_step_func(data_iterator, model)
        else:
            output_tensor, loss_func = forward_step_func(data_iterator, model, checkpoint_activations_microbatch)
    """
    æœ€åä¸€ä¸ªæµæ°´çº¿stageæ—¶è®¡ç®—lossï¼Œ
    éœ€è¦æ³¨æ„çš„æ˜¯ï¼šä¸‹é¢æœ‰ä¸ªlosså’Œloss_reducedï¼Œè¿™ä¸¤ä¸ªlosså‰è€…æ˜¯ç”¨äºåå‘ä¼ æ’­çš„ï¼Œåè€…æ˜¯ç”¨æ¥æ‰“å°æ—¥å¿—çš„ã€‚
    åè€…æœ¬appendè¿›forward_data_storeè¿™ä¸ªå­—å…¸å†…ï¼Œæœ€ç»ˆä¼šè¢«å†æ¬¡å¹³å‡ã€å­—å…¸å†…æ‰€æœ‰lossæ±‚å’Œå†é™¤å­—å…¸é•¿åº¦ã€‘
    """
    if parallel_state.is_pipeline_last_stage():
        if not collect_non_loss_data:
            output_tensor = loss_func(output_tensor)
            loss, loss_reduced = output_tensor
            output_tensor = loss / num_microbatches
            forward_data_store.append(loss_reduced)
        else:
            data = loss_func(output_tensor, non_loss_data=True)
            forward_data_store.append(data)

    ... """ timerè®¡æ—¶å’Œä½¿ç”¨moeä¸‹çš„loss scaleè®¾ç½® """

    if unwrap_output_tensor:
        return output_tensor
    return [output_tensor]
```

åœ¨pipeline_parallel/schedules.pyæ–‡ä»¶ä¸‹çš„forward_step()è¿˜æ˜¯åŒ…è£…äº†çœŸæ­£çš„forward_stepå‡½æ•°ã€å³ä¸Šé¢ä»£ç ä¸­çš„forward_step_funcã€‘ï¼Œè€Œè¿™ä¸ªforward_step_funcå…¶å®å’Œæ¨¡å‹ç±»å‹ç›¸å…³â€”â€”å³ä¸åŒæ¨¡å‹çš„forward_step_funcæœ‰äº›è®¸åŒºåˆ«ï¼Œmegatronä¸ºäº†ç»Ÿä¸€æ¥å£æ‰åŒ…è£…äº†è¿™ä¹ˆå¤šå±‚forward_stepå‡½æ•°ã€‚
ä¸‹é¢æˆ‘ä»¬çœ‹çœ‹gptå’Œt5æ¨¡å‹çš„forward_stepå‡½æ•°ï¼š

```python
def forward_step(data_iterator, model: GPTModel):
    """ GPT Forward training step.
    Args:
        data_iterator : Input data iterator
        model (GPTModel): The GPT Model
    """
    args = get_args()
    timers = get_timers()

    # Get the batch.
    timers('batch-generator', log_level=2).start()
    """ è¿™ä¸ªå…³é”®å‡½æ•°ä»Dataloaderé‡Œè·å–äº†ä¸€ä¸ªbatchçš„æ•°æ® """
    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)  
    timers('batch-generator').stop()

    """ çœŸæ­£çš„forwardè®¡ç®— """ 
    output_tensor = model(tokens, position_ids, attention_mask, labels=labels)

    return output_tensor, partial(loss_func, loss_mask)


def forward_step(data_iterator, model: T5Model):
    """ t5 Forward training step.
    Args:
        data_iterator : Input data iterator
        model (T5Model): The T5 Model
    """
    args = get_args()
    timers = get_timers()

    # Get the batch.
    timers('batch generator', log_level=2).start()
    use_local = args.transformer_impl == "local"
    """ è¿™ä¸ªå…³é”®å‡½æ•°ä»Dataloaderé‡Œè·å–äº†ä¸€ä¸ªbatchçš„æ•°æ® """
    tokens_enc, tokens_dec, loss_mask, lm_labels, enc_mask, dec_mask, enc_dec_mask = get_batch(data_iterator, use_local)
    timers('batch generator').stop()

    """ çœŸæ­£çš„forwardè®¡ç®—, Forward model lm_labels """
    output_tensor = model(tokens_enc, tokens_dec, enc_mask, dec_mask, enc_dec_mask, lm_labels=lm_labels)

    return output_tensor, partial(loss_func, loss_mask)
```

ä»ä¸Šé¢çš„ä»£ç æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç”±äºæ¨¡å‹ä¸åŒï¼Œä¼ å…¥output_tensor = model(...)çš„å‚æ•°ä¹Ÿä¸ä¸€æ ·ï¼Œæ‰€ä»¥forward_stepå°±ä¼šä¸åŒã€‚

è€Œé’ˆå¯¹modelå†…éƒ¨çš„å‰å‘è®¡ç®—æˆ‘ä»¬å°†å…¶æ”¾åœ¨æ–‡ç« æœ€åçš„è¿›é˜¶å†…å®¹ä¸­è®²è¿°ã€‚ã€è®¾è®¡çš„å†…å®¹ä¸»è¦æ˜¯å¼ é‡å¹¶è¡Œï¼Œmegatronå¯¹äºTransformerçš„å¤šå¤´æ³¨æ„åŠ›å’ŒMLPå¼ é‡å¹¶è¡Œç‰ˆæœ¬å®ç°ã€‘ã€‚

## get\_batch:

ä¸Šé¢çš„ä»£ç ä¸­æåˆ°äº†`get_batch()`ã€ä½äº`pretrain_gpt.py`ã€‘æˆ‘ä»¬æ¥ä¸‹æ¥ç€é‡è®²è®²æ•°æ®å¦‚ä½•è¯»å–ï¼š

**ã€megatroné‡Œçš„æ•°æ®å¹¶è¡Œæ˜¯ä¾é æ¯ä¸ªDPåŸŸå†…çš„ç¬¬ä¸€ä¸ªPP stageçš„rank0å¡å„è¯»å–çš„ä¸€æ¬¡æ•°æ®ï¼Œé‚£ä¹ˆNä¸ªDPåŸŸå°±ä¸€å…±ä¼šè¯»å–Nä¸ª`micro_batch_size`å¤§å°çš„æ•°æ®ï¼Œè¿™ä¸ª`N * micro_batch_size`çš„å€æ•°å¯¹åº”äº†`global_batch_size`ã€‘**

```python
def get_batch(data_iterator):
    if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):
        return None, None, None, None, None

    """ ä¸€ä¸ªå¼ é‡å¹¶è¡Œç»„å†…è·å–ä¸€ä¸ªbatchçš„æ•°æ® """
    batch = get_batch_on_this_tp_rank(data_iterator)  

    """ ä¸€ä¸ªä¸Šä¸‹æ–‡å¹¶è¡Œç»„å†…è·å–ä¸€ä¸ªbatchçš„æ•°æ®ï¼ŒåŒæ—¶æŒ‰sequenceç»´åº¦åˆ‡åˆ†è¿™ä¸ªbatchçš„æ•°æ®ç»™åˆ°CPç»„å†…ä¸åŒçš„è®­ç»ƒè¿›ç¨‹ """
    batch = get_batch_on_this_cp_rank(batch) 
    return batch.values()
```

get_batch()å‡½æ•°ç»å†äº†ä¸¤ä¸ªgetçš„è¿‡ç¨‹ï¼Œä¸€ä¸ªæ˜¯TPç»„å†…ï¼Œä¸€ä¸ªæ˜¯CPç»„å†…ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥åˆ†åˆ«è®²è¿°è¿™ä¸¤ä¸ªè¿‡ç¨‹ã€‚

```python
def get_batch_on_this_tp_rank(data_iterator):
    args = get_args()

    def _broadcast(item):
       if item is not None:
           """ 
           broacasté›†åˆé€šä¿¡æ“ä½œçš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯tensoræ•°æ®ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯æ•°æ®çš„å‘é€è€…ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯ç»™å“ªäº›è¿›ç¨‹å‘é€ã€‚
           è¿™é‡Œä½¿ç”¨è¿™ä¸ªé›†åˆé€šä¿¡æ“ä½œçš„åŸå› æ˜¯ä¸€ä¸ªTPç»„å†…åªæœ‰ç»„å†…çš„rank0è¿›ç¨‹æ‰ä¼šä»cpuå†…å­˜ä¾§è¯»å–æ•°æ®é›†åˆ°NPUä¸­ï¼Œç„¶åå…¶
           ä¼šä½¿ç”¨NPUå¡é—´çš„å¹¿æ’­æ“ä½œæŠŠæ•°æ®é›†å¹¿æ’­ç»™ç»„å†…çš„å…¶ä»–NPUã€è¿™æ ·çš„æ•ˆç‡æ¯”æ‰€æœ‰å¡éƒ½ä»cpuå†…å­˜ä¸­è¯»å–è¦å¿«çš„å¤šã€‘
           """
           torch.distributed.broadcast(item, mpu.get_tensor_model_parallel_src_rank(), group=mpu.get_tensor_model_parallel_group())

    if mpu.get_tensor_model_parallel_rank() == 0:
       """ ifçš„æ¡ä»¶æ˜¯TPç»„çš„rank0æ‰ä¼šæ»¡è¶³çš„ï¼Œç„¶ååªæœ‰è¿™ä¸ªè®­ç»ƒè¿›ç¨‹æ‰ä¼šä»data_iteratorä¸­è¯»å–åˆ°dataæ•°æ® """ 
       if data_iterator is not None:
           data = next(data_iterator)
       else:
           data = None
   
       """ ä½¿ç”¨æ•°æ®æ„é€ ä¸€ä¸ªbatch """
       batch = {
           'tokens': data["tokens"].cuda(non_blocking = True),
           'labels': data["labels"].cuda(non_blocking = True),
           'loss_mask': data["loss_mask"].cuda(non_blocking = True),
           'attention_mask': None if "attention_mask" not in data else data["attention_mask"].cuda(non_blocking = True),
           'position_ids': data["position_ids"].cuda(non_blocking = True)
       }
       """ è°ƒç”¨ä¸Šé¢çš„broadcastæ“ä½œæŠŠæ•°æ®å‘é€ç»™ç»„å†…å…¶ä»–çš„rank """
       if args.pipeline_model_parallel_size == 1:
           _broadcast(batch['tokens'])
           _broadcast(batch['labels'])
           _broadcast(batch['loss_mask'])
           _broadcast(batch['attention_mask'])
           _broadcast(batch['position_ids'])
       elif mpu.is_pipeline_first_stage():
           _broadcast(batch['tokens'])
           _broadcast(batch['attention_mask'])
           _broadcast(batch['position_ids'])
       elif mpu.is_pipeline_last_stage():
           _broadcast(batch['labels'])
           _broadcast(batch['loss_mask'])
           _broadcast(batch['attention_mask'])
    else:
       """ ç»„å†…érank0è®­ç»ƒè¿›ç¨‹çš„tokenså’Œlabelséƒ½æ˜¯ç©ºå¼ é‡ï¼Œå…¶éƒ½è°ƒç”¨broadcastæ“ä½œç­‰å¾…rank0æŠŠæ•°æ®å‘ç»™è¿™äº›rank """
       tokens=torch.empty((args.micro_batch_size,args.seq_length), dtype = torch.int64 , device = torch.cuda.current_device())
       labels=torch.empty((args.micro_batch_size,args.seq_length), dtype = torch.int64 , device = torch.cuda.current_device())
       loss_mask=torch.empty((args.micro_batch_size,args.seq_length), dtype = torch.float32 , device = torch.cuda.current_device())
       ...
       position_ids=torch.empty((args.micro_batch_size,args.seq_length), dtype = torch.int64 , device = torch.cuda.current_device())

       if args.pipeline_model_parallel_size == 1:
           _broadcast(tokens)
           _broadcast(labels)
           _broadcast(loss_mask)
           _broadcast(attention_mask)
           _broadcast(position_ids)
       elif mpu.is_pipeline_first_stage():
           labels=None
           loss_mask=None
           _broadcast(tokens)
           _broadcast(attention_mask)
           _broadcast(position_ids)
       elif mpu.is_pipeline_last_stage():
           tokens=None
           position_ids=None
           _broadcast(labels)
           _broadcast(loss_mask)
           _broadcast(attention_mask)
 
       batch = {
           'tokens': tokens,
           'labels': labels,
           'loss_mask': loss_mask,
           'attention_mask': attention_mask,
           'position_ids': position_ids
       }

    return batch

```

æ¥ä¸‹æ¥æˆ‘ä»¬å†çœ‹çœ‹CPçš„batchè¯»å–ï¼š

```python
def get_batch_on_this_cp_rank(batch):
    args = get_args()
    cp_size = args.context_parallel_size
    if cp_size > 1:
        cp_rank = mpu.get_context_parallel_rank()
        for key, val in batch.items():
            if val is not None:
                seq_dim = 1 if key != 'attention_mask' else 2
                """ åªæ‹¿ä¸€éƒ¨åˆ†sequenceé•¿åº¦çš„æ•°æ® """
                val = val.view(
                    *val.shape[0:seq_dim],
                    2 * cp_size,
                    val.shape[seq_dim] // (2 * cp_size),
                    *val.shape[(seq_dim + 1) :],
                )
                index = torch.tensor([cp_rank, (2 * cp_size - cp_rank - 1)], 
                                     device="cpu", pin_memory=True).cuda(non_blocking=True)
                val = val.index_select(seq_dim, index)
                val = val.view(*val.shape[0:seq_dim], -1, *val.shape[(seq_dim + 2) :])
                batch[key] = val

    return batch
```

å¯¹äºCPç»„çš„batchè€Œè¨€ä¹Ÿå¾ˆå®¹æ˜“ç†è§£ï¼Œä¼ å…¥è¿™ä¸ªå‡½æ•°çš„batchæ˜¯ä»TPç»„è·å¾—çš„batchä¸­ï¼Œè¿™ä¸ªCPrankå†…çš„å¡æŠŠä¸€ä¸ªå®Œæˆsequenceé•¿åº¦çš„batchæ•°æ®æŠ½å‡ºå±äºè‡ªå·±çš„ä¸€éƒ¨åˆ†ï¼Œç„¶åå…¶ä½™éƒ¨åˆ†ä¸¢å¼ƒã€‚åŒæ—¶å€¼å¾—æ³¨æ„çš„æ˜¯sequenceä¼šè¢«åˆ‡åˆ†ä¸ºä¸¤å€çš„CPå¤§å°çš„chunkï¼Œè¿™æ ·åšä¸»è¦æ˜¯ä¸ºäº†è´Ÿè½½å‡è¡¡ã€‚

çœ‹åˆ°è¿™é‡Œå†ç»“åˆç¬¬äºŒç« å¹¶è¡Œåˆå§‹åŒ–çš„çŸ¥è¯†ï¼Œæˆ‘ç›¸ä¿¡ä½ ä¸€å®šæœ‰ä¸ªç–‘é—®â€”â€”ä¸¤ä¸ªCPå¹¶è¡ŒåŸŸå†…æ˜¯å¦‚ä½•ç¡®ä¿æ‹¿åˆ°çš„æ˜¯åŒä¸€ä¸ªbatchçš„æ•°æ®çš„å‘¢ï¼Ÿ

ç­”æ¡ˆï¼šå¾—ç›Šäºmegatronçš„samplerçš„è®¾è®¡ï¼Œç”±äºä¸¤ä¸ªCPå¹¶è¡ŒåŸŸçš„DP rankæ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥åœ¨next(data_iterator)çš„æ—¶å€™è°ƒç”¨__iter__å‡½æ•°yieldçš„batch idxæ˜¯ä¸€æ ·çš„ï¼Œå¦‚æ­¤æ‰èƒ½å¤Ÿä¿è¯CPå¹¶è¡ŒåŸŸå†…çš„æ•°æ®æ˜¯åŒä¸€æ‰¹ã€‚

ã€ä»£ç ç»†èŠ‚å¯ä»¥åœ¨megatron/legacy/data/data_sampler.pyä¸­çœ‹åˆ°ã€‘

## backward\_stepï¼š

ä¸Šä¸€èŠ‚çš„`forward_step`å’Œ`get_batch`åŒ…è£…äº†å¾ˆå¤šå¾ˆå¤šå±‚ï¼Œä½†`backward_step`å°±ä¼šå¥½å¾ˆå¤š

```python
def backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config):
  
    unwrap_input_tensor_grad = False
    if not isinstance(input_tensor, list):
        input_tensor = [input_tensor]
        unwrap_input_tensor_grad = True
    for x in input_tensor:
        if x is not None:
            x.retain_grad()

    if not isinstance(output_tensor, list):
        output_tensor = [output_tensor]
    if not isinstance(output_tensor_grad, list):
        output_tensor_grad = [output_tensor_grad]

    """ Backward pass. æ ¹æ®output_tensorå’Œoutput_tensor_gradè®¡ç®—å‡ºinput_tensorçš„æ¢¯åº¦ """
    if output_tensor_grad[0] is None and config.grad_scale_func is not None:
        output_tensor[0] = config.grad_scale_func(output_tensor[0])

    if config.deallocate_pipeline_outputs:
        custom_backward(output_tensor[0], output_tensor_grad[0])
    else:
        torch.autograd.backward(output_tensor[0], grad_tensors=output_tensor_grad[0])

    # Collect the grad of the input_tensor.
    input_tensor_grad = [None]
    if input_tensor is not None:
        input_tensor_grad = []
        for x in input_tensor:
            if x is None:
                input_tensor_grad.append(None)
            else:
                input_tensor_grad.append(x.grad)

    ... """ skip layerç›¸å…³çš„é€»è¾‘ """ 

    return input_tensor_grad
```

# åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ï¼š

megatronçš„åˆ†å¸ƒå¼ä¼˜åŒ–å™¨æ˜¯ååˆ†é‡è¦çš„ä¸€ä¸ªç»„ä»¶ï¼Œå…¶æ—¢æ˜¯åˆ†å¸ƒå¼çš„åˆæ˜¯æ··åˆç²¾åº¦çš„ï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨forward-backwardã€stepçš„è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ¨¡å‹å‚æ•°çš„ç²¾åº¦æ˜¯ä¸ä¸€æ ·çš„ã€‚ä¸‹é¢è¿™å¼ å›¾å°†å±•ç¤ºå‡ºæ¥å„æ­¥éª¤ç”¨çš„ç²¾åº¦ï¼š
![img_2.png](img_2.png)
![img_3.png](img_3.png)
ä¸Šå›¾çš„æµç¨‹å¤§è‡´ä¸ºå¦‚ä¸‹é€»è¾‘ï¼š



ä¸Šå›¾çš„æµç¨‹å¤§è‡´ä¸ºå¦‚ä¸‹é€»è¾‘ï¼š

* ä»`main_params`å°†æƒé‡å‚æ•°æ‹·è´åˆ°`model_params`ï¼Œç„¶åä½¿ç”¨`model_params`è¿›è¡Œ`forward`è®¡ç®—ã€è¿™é‡Œçš„`main_params`ä¿å­˜çš„æƒé‡å‚æ•°çš„ç²¾åº¦ä¸ºFP32ï¼Œè€Œ`model_params`çš„ç²¾åº¦ä¸ºFP16ã€‘
* å°†forwardçš„Lossç”¨äºbackwardæ¥è®¡ç®—æ¢¯åº¦`scaled_grad`ã€è¿™é‡Œçš„gradçš„ç²¾åº¦ä¹Ÿæ˜¯FP16ã€‘
* å°†scaled\_gradã€FP16ã€‘æ‹·è´åˆ°è®­ç»ƒè¿›ç¨‹çš„grad bufferå†…ï¼Œè¿›è¡Œ`reduce-scatter`ï¼Œè®©è¿™ä¸ªrankæŒæœ‰çš„æ¨¡å‹å‚æ•°çš„DPç»„å†…æœ‰ä¸€ä¸ªrankèƒ½æœ‰ä¸€ä¸ªbatchæ±‡æ€»å¹³å‡è¿‡çš„æ¢¯åº¦ã€FP16ã€‘
* å°†ä¸Šä¸€æ­¥`reduce-scatter`æ±‡æ€»è¿‡çš„æ¢¯åº¦ã€FP16ã€‘unscaleä¸ºç²¾åº¦ä¸ºFP32çš„æ¢¯åº¦ï¼ˆunscaleæ—¶è¿˜éœ€è¦è¿›è¡Œclipä¹‹ç±»çš„æ¢¯åº¦ç¼©æ”¾æ“ä½œï¼Œè®©æ¢¯åº¦ä¸è¦è¿‡å¤§ã€é¿å…æ¢¯åº¦çˆ†ç‚¸ã€‘ï¼‰ï¼Œç„¶åæ‰§è¡Œ`optimizer.step()`å¯¹**è¿™ä¸ªrankæ‰€æŒæœ‰çš„åˆ†å¸ƒå¼ä¼˜åŒ–å™¨å†…æŒæœ‰çš„ä¼˜åŒ–å™¨çŠ¶æ€çš„shardæ‰€æ‹¥æœ‰çš„æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°**ã€è¿™é‡Œå¾ˆç»•ï¼Œéœ€è¦è¯»è€…å¥½å¥½æ‹ä¸€æ‹ï¼Œä¸‹é¢åˆ†å¸ƒå¼ä¼˜åŒ–å™¨æ›´æ–°è¿‡ç¨‹é‡Œä¼šæœ‰è¿›ä¸€æ­¥çš„é˜è¿°ï¼Œä½†æˆ‘ä»¬åœ¨è¿™é‡Œéœ€è¦è®°ä½ä¸€ä¸ªæ¦‚å¿µâ€”â€”**ç”±äºæ¨¡å‹å¹¶è¡Œçš„å­˜åœ¨ï¼Œæ¯ä¸ªrankå†…çš„modelåªæ˜¯å®Œæ•´modelçš„ä¸€ä¸ªç¢ç‰‡**ã€‘
* æ‰€æœ‰æŒæœ‰ç›¸åŒå®Œæ•´æ¨¡å‹çš„ç¢ç‰‡çš„rankã€å³ä¸€ä¸ªDPé€šè®¯ç»„é‡Œçš„æ‰€æœ‰rankã€‘è°ƒç”¨`all-gather`å°†ä¸Šä¸€æ­¥æ›´æ–°è¿‡çš„éƒ¨åˆ†å‚æ•°æ±‡æ€»èµ·æ¥ï¼Œè¿™æ ·è¿™ä¸ªDPç»„é‡Œçš„æ‰€æœ‰rankå°±éƒ½æœ‰è¿™ä¸ªmodelç¢ç‰‡çš„å®Œæ•´çš„æ›´æ–°è¿‡çš„å‚æ•°äº†
